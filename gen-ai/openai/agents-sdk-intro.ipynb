{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aurelio-labs/cookbook/blob/main/gen-ai/openai/agents-sdk-intro.ipynb)\n"
      ],
      "metadata": {
        "id": "d1DGYFfoKvsB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI's Agents SDK"
      ],
      "metadata": {
        "id": "sG6pIZ-_7uRX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenAI have released an **Agents SDK**, their version of an open source agent development library.\n",
        "\n",
        "OpenAI have outlined a few features of the library:\n",
        "\n",
        "```\n",
        "* Agent loop: Built-in agent loop that handles calling tools, sending results to the LLM, and looping until the LLM is done.\n",
        "* Python-first: Use built-in language features to orchestrate and chain agents, rather than needing to learn new abstractions.\n",
        "* Handoffs: A powerful feature to coordinate and delegate between multiple agents.\n",
        "* Guardrails: Run input validations and checks in parallel to your agents, breaking early if the checks fail.\n",
        "* Function tools: Turn any Python function into a tool, with automatic schema generation and Pydantic-powered validation.\n",
        "* Tracing: Built-in tracing that lets you visualize, debug and monitor your workflows, as well as use the OpenAI suite of evaluation, fine-tuning and distillation tools.\n",
        "```\n",
        "\n",
        "([source](https://openai.github.io/openai-agents-python/))\n",
        "\n",
        "We'll focus on covering the essentials here - including the **agent loop**, **python-first**, **guardrails**, and **function tools** features.\n",
        "\n",
        "Let's start by installing the library:"
      ],
      "metadata": {
        "id": "rN2gNfry7tDp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3aqSlMc27noo",
        "outputId": "4f5bb325-a199-42b6-a945-e7c25a3959b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/75.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m71.7/75.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.5/75.5 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU openai-agents==0.0.3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let's set our [OpenAI API key](https://platform.openai.com/settings/organization/api-keys)."
      ],
      "metadata": {
        "id": "nBGHkh7nBVHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or \\\n",
        "  getpass(\"Enter your OpenAI API key: \")"
      ],
      "metadata": {
        "id": "Kwpk_H5_BnAl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f428812f-7f4a-4313-bae3-cb6254eb54b1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Agent, Runner\n",
        "\n",
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=\"You're a helpful assistant\",\n",
        "    model=\"gpt-4o-mini\",\n",
        ")"
      ],
      "metadata": {
        "id": "lqkaleTAAjHK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running our Agent"
      ],
      "metadata": {
        "id": "0-zOBFhAEROm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenAI gives us three methods for running our agent, all via a `Runner` class — those methods are:\n",
        "\n",
        "1. `Runner.run()` which runs in async.\n",
        "2. `Runner.run_sync()` which runs in sync.\n",
        "3. `Runner.run_streamed()` which runs in async _and_ streams the response back to us.\n",
        "\n",
        "We'll quicky test method **(1)**:"
      ],
      "metadata": {
        "id": "dWO4ozC8ETNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = await Runner.run(\n",
        "    starting_agent=agent,\n",
        "    input=\"tell me a short story\"\n",
        ")\n",
        "result.final_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "Bjn-gyR9BIn-",
        "outputId": "aa0eb834-af15-4049-d7ec-b8d18ebdc30f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:openai.agents:Error getting response: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-895f37052f1b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m result = await Runner.run(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mstarting_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tell me a short story\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/agents/run.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(cls, starting_agent, input, context, max_turns, hooks, run_config)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mcurrent_turn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m                         input_guardrail_results, turn_result = await asyncio.gather(\n\u001b[0m\u001b[1;32m    211\u001b[0m                             cls._run_input_guardrails(\n\u001b[1;32m    212\u001b[0m                                 \u001b[0mstarting_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/agents/run.py\u001b[0m in \u001b[0;36m_run_single_turn\u001b[0;34m(cls, agent, original_input, generated_items, hooks, context_wrapper, run_config, should_run_agent_start_hooks)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgenerated_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_input_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgenerated_item\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerated_items\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m         new_response = await cls._get_new_response(\n\u001b[0m\u001b[1;32m    720\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m             \u001b[0msystem_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/agents/run.py\u001b[0m in \u001b[0;36m_get_new_response\u001b[0;34m(cls, agent, system_prompt, input, output_schema, handoffs, context_wrapper, run_config)\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0mmodel_settings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_settings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m         new_response = await model.get_response(\n\u001b[0m\u001b[1;32m    863\u001b[0m             \u001b[0msystem_instructions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msystem_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/agents/models/openai_responses.py\u001b[0m in \u001b[0;36mget_response\u001b[0;34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, tracing)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mresponse_span\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtracing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_disabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mspan_response\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 response = await self._fetch_response(\n\u001b[0m\u001b[1;32m     76\u001b[0m                     \u001b[0msystem_instructions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                     \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/agents/models/openai_responses.py\u001b[0m in \u001b[0;36m_fetch_response\u001b[0;34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, stream)\u001b[0m\n\u001b[1;32m    227\u001b[0m             )\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         return await self._client.responses.create(\n\u001b[0m\u001b[1;32m    230\u001b[0m             \u001b[0minstructions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_non_null_or_not_given\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msystem_instructions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/responses/responses.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, input, model, include, instructions, max_output_tokens, metadata, parallel_tool_calls, previous_response_id, reasoning, service_tier, store, stream, temperature, text, tool_choice, tools, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1557\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mNotGiven\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNOT_GIVEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1558\u001b[0m     ) -> Response | AsyncStream[ResponseStreamEvent]:\n\u001b[0;32m-> 1559\u001b[0;31m         return await self._post(\n\u001b[0m\u001b[1;32m   1560\u001b[0m             \u001b[0;34m\"/responses\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m             body=await async_maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1740\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mawait\u001b[0m \u001b[0masync_to_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m         )\n\u001b[0;32m-> 1742\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1744\u001b[0m     async def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1548\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1549\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In most scenarios we'll likely want to be using method **(3)**, ie running async and streaming tokens. To do this we need to write a little more code to handle the async streaming and print the tokens as they're returned.\n",
        "\n",
        "First, we create a `RunResultStreaming` object by calling `Runner.run_streamed(...)`, we then _asynchronously_ iterate through the streamed events returned by our LLM using the `response.stream_events()` method:"
      ],
      "metadata": {
        "id": "W5_yue8fE4eX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = Runner.run_streamed(\n",
        "    starting_agent=agent,\n",
        "    input=\"hello there\"\n",
        ")\n",
        "async for event in response.stream_events():\n",
        "    print(event)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBnhUepDBPHM",
        "outputId": "043f936a-7aea-4ec8-fdfd-a7aeb9ea2f39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AgentUpdatedStreamEvent(new_agent=Agent(name='Assistant', instructions=\"You're a helpful assistant\", handoff_description=None, handoffs=[], model='gpt-4o-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None), tools=[], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None), type='agent_updated_stream_event')\n",
            "RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='resp_67d1798523648191be119fb7699b81c5018faad15bf78d42', created_at=1741781381.0, error=None, incomplete_details=None, instructions=\"You're a helpful assistant\", metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None), status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=None, user=None, store=True), type='response.created'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseInProgressEvent(response=Response(id='resp_67d1798523648191be119fb7699b81c5018faad15bf78d42', created_at=1741781381.0, error=None, incomplete_details=None, instructions=\"You're a helpful assistant\", metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None), status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=None, user=None, store=True), type='response.in_progress'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='msg_67d1798580e08191a76aef549fa24cc1018faad15bf78d42', content=[], role='assistant', status='in_progress', type='message'), output_index=0, type='response.output_item.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseContentPartAddedEvent(content_index=0, item_id='msg_67d1798580e08191a76aef549fa24cc1018faad15bf78d42', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text'), type='response.content_part.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='Hello', item_id='msg_67d1798580e08191a76aef549fa24cc1018faad15bf78d42', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='!', item_id='msg_67d1798580e08191a76aef549fa24cc1018faad15bf78d42', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' How', item_id='msg_67d1798580e08191a76aef549fa24cc1018faad15bf78d42', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' can', item_id='msg_67d1798580e08191a76aef549fa24cc1018faad15bf78d42', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' I', item_id='msg_67d1798580e08191a76aef549fa24cc1018faad15bf78d42', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' assist', item_id='msg_67d1798580e08191a76aef549fa24cc1018faad15bf78d42', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' you', item_id='msg_67d1798580e08191a76aef549fa24cc1018faad15bf78d42', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' today', item_id='msg_67d1798580e08191a76aef549fa24cc1018faad15bf78d42', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='?', item_id='msg_67d1798580e08191a76aef549fa24cc1018faad15bf78d42', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDoneEvent(content_index=0, item_id='msg_67d1798580e08191a76aef549fa24cc1018faad15bf78d42', output_index=0, text='Hello! How can I assist you today?', type='response.output_text.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseContentPartDoneEvent(content_index=0, item_id='msg_67d1798580e08191a76aef549fa24cc1018faad15bf78d42', output_index=0, part=ResponseOutputText(annotations=[], text='Hello! How can I assist you today?', type='output_text'), type='response.content_part.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='msg_67d1798580e08191a76aef549fa24cc1018faad15bf78d42', content=[ResponseOutputText(annotations=[], text='Hello! How can I assist you today?', type='output_text')], role='assistant', status='completed', type='message'), output_index=0, type='response.output_item.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='resp_67d1798523648191be119fb7699b81c5018faad15bf78d42', created_at=1741781381.0, error=None, incomplete_details=None, instructions=\"You're a helpful assistant\", metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[ResponseOutputMessage(id='msg_67d1798580e08191a76aef549fa24cc1018faad15bf78d42', content=[ResponseOutputText(annotations=[], text='Hello! How can I assist you today?', type='output_text')], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None), status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=ResponseUsage(input_tokens=35, output_tokens=10, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=45, input_tokens_details={'cached_tokens': 0}), user=None, store=True), type='response.completed'), type='raw_response_event')\n",
            "RunItemStreamEvent(name='message_output_created', item=MessageOutputItem(agent=Agent(name='Assistant', instructions=\"You're a helpful assistant\", handoff_description=None, handoffs=[], model='gpt-4o-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None), tools=[], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None), raw_item=ResponseOutputMessage(id='msg_67d1798580e08191a76aef549fa24cc1018faad15bf78d42', content=[ResponseOutputText(annotations=[], text='Hello! How can I assist you today?', type='output_text')], role='assistant', status='completed', type='message'), type='message_output_item'), type='run_item_stream_event')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can filter these various event types to find only raw tokens like so:"
      ],
      "metadata": {
        "id": "iH-RoQfzMj6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai.types.responses import ResponseTextDeltaEvent\n",
        "\n",
        "# we do need to reinitialize our runner before re-executing\n",
        "response = Runner.run_streamed(\n",
        "    starting_agent=agent,\n",
        "    input=\"tell me a short story\"\n",
        ")\n",
        "\n",
        "async for event in response.stream_events():\n",
        "    if event.type == \"raw_response_event\" and \\\n",
        "        isinstance(event.data, ResponseTextDeltaEvent):\n",
        "        print(event.data.delta, end=\"\", flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGQANyvXG9zJ",
        "outputId": "8f64469d-fe14-4d74-be40-6a0d0af51848"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once in a small village nestled between rolling hills, there lived an old clockmaker named Eli. His workshop was filled with clocks of all sizes, each ticking in harmony, creating a melody of time. Eli was beloved in the village not just for his craftsmanship, but for the way he listened to people’s stories as they waited for their clocks to be repaired.\n",
            "\n",
            "One day, a young girl named Lila came into the shop, her eyes shimmering with curiosity. She had a broken pocket watch that belonged to her grandmother. As Eli worked on it, Lila shared tales of her grandmother’s adventures, filling the room with laughter and warmth.\n",
            "\n",
            "When he finished, Eli handed the watch back, but not before doing something special. He added a tiny, hidden mechanism that would play a gentle chime whenever Lila opened it. “This way,” he said with a smile, “you’ll always remember the moments that matter.”\n",
            "\n",
            "Years passed, and Lila grew up, treasure in hand. Whenever she felt lost or concerned, she would open the watch, and the sweet chime would echo, reminding her of her grandmother's stories and love.\n",
            "\n",
            "One day, Lila returned to the shop, now as a woman with stories of her own. She found Eli, who was gray but still had that twinkle in his eye. \n",
            "\n",
            "“Thank you for the music,” she said, showing him the pocket watch. “It carries all our memories.”\n",
            "\n",
            "Eli nodded, knowing that sometimes, the most precious things are not just the clocks that tell time, but the moments they help us cherish. And in that small workshop, the melody of life continued to tick on, weaving the tapestry of memories for all who entered."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tools"
      ],
      "metadata": {
        "id": "9Iixlv0sRMqr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenAI included **function tools** as a key feature in their Agents SDK announcement. After turning everyone away from using _function calling_ to instead use _tool calling_, OpenAI have now decided that an LLM deciding to execute some code will be called _\"function tools\"_.\n",
        "\n",
        "To use _function tools_ in Agents SDK we simply decorate a function with the `@function_tool` decorator like so:"
      ],
      "metadata": {
        "id": "kDHORYUcROZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import function_tool\n",
        "\n",
        "@function_tool\n",
        "def multiply(x: float, y: float) -> float:\n",
        "    \"\"\"Multiplies `x` and `y` to provide a precise\n",
        "    answer.\"\"\"\n",
        "    return x*y"
      ],
      "metadata": {
        "id": "tJz7BlwiNOa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we have taken extra care to include a clear and descriptive function name, relatively clear parameter names, type annotations for both input parameters and expected output, and a natural language docstring that will be fed to the LLM and explain to it _what_ this tool does.\n",
        "\n",
        "To run our agent _with_ tools we simply pass our new tool into the `tools` parameter during `Agent` initialization."
      ],
      "metadata": {
        "id": "I8b8KektTQbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=(\n",
        "        \"You're a helpful assistant, remember to always \"\n",
        "        \"use the provided tools whenever possible. Do not \"\n",
        "        \"rely on your own knowledge too much and instead \"\n",
        "        \"use your tools to help you answer queries.\"\n",
        "    ),\n",
        "    model=\"gpt-4o-mini\",\n",
        "    tools=[multiply]  # note that we expect a list of tools\n",
        ")"
      ],
      "metadata": {
        "id": "WnT4C9WBTO_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's initialize a new runner and execute our agent with tools:"
      ],
      "metadata": {
        "id": "VtioNxflUS-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = Runner.run_streamed(\n",
        "    starting_agent=agent,\n",
        "    input=\"what is 7.814 multiplied by 103.892?\"\n",
        ")\n",
        "\n",
        "async for event in response.stream_events():\n",
        "    print(event)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSOREQ-XUSVi",
        "outputId": "579e3dc2-832c-4d35-da30-5bcbd757248b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AgentUpdatedStreamEvent(new_agent=Agent(name='Assistant', instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", handoff_description=None, handoffs=[], model='gpt-4o-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None), tools=[FunctionTool(name='multiply', description='Multiplies `x` and `y` to provide a precise\\nanswer.', params_json_schema={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7e5dbc17c220>, strict_json_schema=True)], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None), type='agent_updated_stream_event')\n",
            "RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='resp_67d17a91805881919555abb63f3bafb30b5820618b240096', created_at=1741781649.0, error=None, incomplete_details=None, instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='multiply', parameters={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, strict=True, type='function', description='Multiplies `x` and `y` to provide a precise\\nanswer.')], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None), status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=None, user=None, store=True), type='response.created'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseInProgressEvent(response=Response(id='resp_67d17a91805881919555abb63f3bafb30b5820618b240096', created_at=1741781649.0, error=None, incomplete_details=None, instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='multiply', parameters={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, strict=True, type='function', description='Multiplies `x` and `y` to provide a precise\\nanswer.')], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None), status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=None, user=None, store=True), type='response.in_progress'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseFunctionToolCall(id='fc_67d17a9203d48191a1443a602128bbbf0b5820618b240096', arguments='', call_id='call_ohmuulIoBjnbISiGqrHqK9qw', name='multiply', type='function_call', status='in_progress'), output_index=0, type='response.output_item.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='{\"', item_id='fc_67d17a9203d48191a1443a602128bbbf0b5820618b240096', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='x', item_id='fc_67d17a9203d48191a1443a602128bbbf0b5820618b240096', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='\":', item_id='fc_67d17a9203d48191a1443a602128bbbf0b5820618b240096', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='7', item_id='fc_67d17a9203d48191a1443a602128bbbf0b5820618b240096', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='.', item_id='fc_67d17a9203d48191a1443a602128bbbf0b5820618b240096', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='814', item_id='fc_67d17a9203d48191a1443a602128bbbf0b5820618b240096', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta=',\"', item_id='fc_67d17a9203d48191a1443a602128bbbf0b5820618b240096', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='y', item_id='fc_67d17a9203d48191a1443a602128bbbf0b5820618b240096', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='\":', item_id='fc_67d17a9203d48191a1443a602128bbbf0b5820618b240096', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='103', item_id='fc_67d17a9203d48191a1443a602128bbbf0b5820618b240096', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='.', item_id='fc_67d17a9203d48191a1443a602128bbbf0b5820618b240096', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='892', item_id='fc_67d17a9203d48191a1443a602128bbbf0b5820618b240096', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='}', item_id='fc_67d17a9203d48191a1443a602128bbbf0b5820618b240096', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDoneEvent(arguments='{\"x\":7.814,\"y\":103.892}', item_id='fc_67d17a9203d48191a1443a602128bbbf0b5820618b240096', output_index=0, type='response.function_call_arguments.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseFunctionToolCall(id='fc_67d17a9203d48191a1443a602128bbbf0b5820618b240096', arguments='{\"x\":7.814,\"y\":103.892}', call_id='call_ohmuulIoBjnbISiGqrHqK9qw', name='multiply', type='function_call', status='completed'), output_index=0, type='response.output_item.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='resp_67d17a91805881919555abb63f3bafb30b5820618b240096', created_at=1741781649.0, error=None, incomplete_details=None, instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[ResponseFunctionToolCall(id='fc_67d17a9203d48191a1443a602128bbbf0b5820618b240096', arguments='{\"x\":7.814,\"y\":103.892}', call_id='call_ohmuulIoBjnbISiGqrHqK9qw', name='multiply', type='function_call', status='completed')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='multiply', parameters={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, strict=True, type='function', description='Multiplies `x` and `y` to provide a precise\\nanswer.')], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None), status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=ResponseUsage(input_tokens=329, output_tokens=22, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=351, input_tokens_details={'cached_tokens': 0}), user=None, store=True), type='response.completed'), type='raw_response_event')\n",
            "RunItemStreamEvent(name='tool_called', item=ToolCallItem(agent=Agent(name='Assistant', instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", handoff_description=None, handoffs=[], model='gpt-4o-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None), tools=[FunctionTool(name='multiply', description='Multiplies `x` and `y` to provide a precise\\nanswer.', params_json_schema={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7e5dbc17c220>, strict_json_schema=True)], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None), raw_item=ResponseFunctionToolCall(id='fc_67d17a9203d48191a1443a602128bbbf0b5820618b240096', arguments='{\"x\":7.814,\"y\":103.892}', call_id='call_ohmuulIoBjnbISiGqrHqK9qw', name='multiply', type='function_call', status='completed'), type='tool_call_item'), type='run_item_stream_event')\n",
            "RunItemStreamEvent(name='tool_output', item=ToolCallOutputItem(agent=Agent(name='Assistant', instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", handoff_description=None, handoffs=[], model='gpt-4o-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None), tools=[FunctionTool(name='multiply', description='Multiplies `x` and `y` to provide a precise\\nanswer.', params_json_schema={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7e5dbc17c220>, strict_json_schema=True)], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None), raw_item={'call_id': 'call_ohmuulIoBjnbISiGqrHqK9qw', 'output': '811.812088', 'type': 'function_call_output'}, output='811.812088', type='tool_call_output_item'), type='run_item_stream_event')\n",
            "RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='resp_67d17a9274f88191bf4e536891e8edb50b5820618b240096', created_at=1741781650.0, error=None, incomplete_details=None, instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='multiply', parameters={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, strict=True, type='function', description='Multiplies `x` and `y` to provide a precise\\nanswer.')], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None), status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=None, user=None, store=True), type='response.created'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseInProgressEvent(response=Response(id='resp_67d17a9274f88191bf4e536891e8edb50b5820618b240096', created_at=1741781650.0, error=None, incomplete_details=None, instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='multiply', parameters={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, strict=True, type='function', description='Multiplies `x` and `y` to provide a precise\\nanswer.')], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None), status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=None, user=None, store=True), type='response.in_progress'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='msg_67d17a92f6348191930a2b7c8a42a9990b5820618b240096', content=[], role='assistant', status='in_progress', type='message'), output_index=0, type='response.output_item.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseContentPartAddedEvent(content_index=0, item_id='msg_67d17a92f6348191930a2b7c8a42a9990b5820618b240096', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text'), type='response.content_part.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='The', item_id='msg_67d17a92f6348191930a2b7c8a42a9990b5820618b240096', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' result', item_id='msg_67d17a92f6348191930a2b7c8a42a9990b5820618b240096', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' of', item_id='msg_67d17a92f6348191930a2b7c8a42a9990b5820618b240096', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' multiplying', item_id='msg_67d17a92f6348191930a2b7c8a42a9990b5820618b240096', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' ', item_id='msg_67d17a92f6348191930a2b7c8a42a9990b5820618b240096', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='7', item_id='msg_67d17a92f6348191930a2b7c8a42a9990b5820618b240096', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='.', item_id='msg_67d17a92f6348191930a2b7c8a42a9990b5820618b240096', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='814', item_id='msg_67d17a92f6348191930a2b7c8a42a9990b5820618b240096', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' by', item_id='msg_67d17a92f6348191930a2b7c8a42a9990b5820618b240096', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' ', item_id='msg_67d17a92f6348191930a2b7c8a42a9990b5820618b240096', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='103', item_id='msg_67d17a92f6348191930a2b7c8a42a9990b5820618b240096', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='.', item_id='msg_67d17a92f6348191930a2b7c8a42a9990b5820618b240096', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='892', item_id='msg_67d17a92f6348191930a2b7c8a42a9990b5820618b240096', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' is', item_id='msg_67d17a92f6348191930a2b7c8a42a9990b5820618b240096', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' approximately', item_id='msg_67d17a92f6348191930a2b7c8a42a9990b5820618b240096', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' ', item_id='msg_67d17a92f6348191930a2b7c8a42a9990b5820618b240096', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='811', item_id='msg_67d17a92f6348191930a2b7c8a42a9990b5820618b240096', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='.', item_id='msg_67d17a92f6348191930a2b7c8a42a9990b5820618b240096', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='812', item_id='msg_67d17a92f6348191930a2b7c8a42a9990b5820618b240096', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='.', item_id='msg_67d17a92f6348191930a2b7c8a42a9990b5820618b240096', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDoneEvent(content_index=0, item_id='msg_67d17a92f6348191930a2b7c8a42a9990b5820618b240096', output_index=0, text='The result of multiplying 7.814 by 103.892 is approximately 811.812.', type='response.output_text.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseContentPartDoneEvent(content_index=0, item_id='msg_67d17a92f6348191930a2b7c8a42a9990b5820618b240096', output_index=0, part=ResponseOutputText(annotations=[], text='The result of multiplying 7.814 by 103.892 is approximately 811.812.', type='output_text'), type='response.content_part.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='msg_67d17a92f6348191930a2b7c8a42a9990b5820618b240096', content=[ResponseOutputText(annotations=[], text='The result of multiplying 7.814 by 103.892 is approximately 811.812.', type='output_text')], role='assistant', status='completed', type='message'), output_index=0, type='response.output_item.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='resp_67d17a9274f88191bf4e536891e8edb50b5820618b240096', created_at=1741781650.0, error=None, incomplete_details=None, instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[ResponseOutputMessage(id='msg_67d17a92f6348191930a2b7c8a42a9990b5820618b240096', content=[ResponseOutputText(annotations=[], text='The result of multiplying 7.814 by 103.892 is approximately 811.812.', type='output_text')], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='multiply', parameters={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, strict=True, type='function', description='Multiplies `x` and `y` to provide a precise\\nanswer.')], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None), status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=ResponseUsage(input_tokens=362, output_tokens=22, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=384, input_tokens_details={'cached_tokens': 0}), user=None, store=True), type='response.completed'), type='raw_response_event')\n",
            "RunItemStreamEvent(name='message_output_created', item=MessageOutputItem(agent=Agent(name='Assistant', instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", handoff_description=None, handoffs=[], model='gpt-4o-mini', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None), tools=[FunctionTool(name='multiply', description='Multiplies `x` and `y` to provide a precise\\nanswer.', params_json_schema={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7e5dbc17c220>, strict_json_schema=True)], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None), raw_item=ResponseOutputMessage(id='msg_67d17a92f6348191930a2b7c8a42a9990b5820618b240096', content=[ResponseOutputText(annotations=[], text='The result of multiplying 7.814 by 103.892 is approximately 811.812.', type='output_text')], role='assistant', status='completed', type='message'), type='message_output_item'), type='run_item_stream_event')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we look closely at the fourth event object we will see `ResponseFunctionToolCall`, meaning our `multiply` tool was called by our LLM. Following this event object we can also see several events containing the `ResponseFunctionCallArgumentsDeltaEvent` type inside the `data` field — these are the input parameters for our tool.\n",
        "\n",
        "Let's rerun that but this time we will process the event outputs to generate a cleaner and more readable output."
      ],
      "metadata": {
        "id": "jQZu6iyPU0wG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai.types.responses import (\n",
        "    ResponseFunctionCallArgumentsDeltaEvent,  # tool call streaming\n",
        "    ResponseCreatedEvent,  # start of new event like tool call or final answer\n",
        ")\n",
        "\n",
        "response = Runner.run_streamed(\n",
        "    starting_agent=agent,\n",
        "    input=\"what is 7.814 multiplied by 103.892?\"\n",
        ")\n",
        "\n",
        "async for event in response.stream_events():\n",
        "    if event.type == \"raw_response_event\":\n",
        "        if isinstance(event.data, ResponseFunctionCallArgumentsDeltaEvent):\n",
        "            # this is streamed parameters for our tool call\n",
        "            print(event.data.delta, end=\"\", flush=True)\n",
        "        elif isinstance(event.data, ResponseTextDeltaEvent):\n",
        "            # this is streamed final answer tokens\n",
        "            print(event.data.delta, end=\"\", flush=True)\n",
        "    elif event.type == \"agent_updated_stream_event\":\n",
        "        # this tells us which agent is currently in use\n",
        "        print(f\"> Current Agent: {event.new_agent.name}\")\n",
        "    elif event.type == \"run_item_stream_event\":\n",
        "        # these are events containing info that we'd typically\n",
        "        # stream out to a user or some downstream process\n",
        "        if event.name == \"tool_called\":\n",
        "            # this is the collection of our _full_ tool call after our tool\n",
        "            # tokens have all been streamed\n",
        "            print()\n",
        "            print(f\"> Tool Called, name: {event.item.raw_item.name}\")\n",
        "            print(f\"> Tool Called, args: {event.item.raw_item.arguments}\")\n",
        "        elif event.name == \"tool_output\":\n",
        "            # this is the response from our tool execution\n",
        "            print(f\"> Tool Output: {event.item.raw_item['output']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhFvNVowUrd5",
        "outputId": "c572523c-e86a-4925-9010-367387766b5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Current Agent: Assistant\n",
            "{\"x\":7.814,\"y\":103.892}\n",
            "> Tool Called, name: multiply\n",
            "> Tool Called, args: {\"x\":7.814,\"y\":103.892}\n",
            "> Tool Output: 811.812088\n",
            "The result of multiplying 7.814 by 103.892 is approximately 811.812."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Guardrails"
      ],
      "metadata": {
        "id": "CqqWET4O93il"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenAI have also included guardrails in the Agents SDK. These come as _input guardrails_ and _output guardrails_, the `input_guardrail` checks that the input going into your LLM is \"safe\" and the `output_guardrail` checks that the output from your LLM is \"safe\".\n",
        "\n",
        "Let's see how to use them. First, we'll implement a guardrail powered by another LLM (more tokens means more $$$ for OpenAI)."
      ],
      "metadata": {
        "id": "CBrf2Ao396Dg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "# define structure of output for any guardrail agents\n",
        "class GuardrailOutput(BaseModel):\n",
        "    is_triggered: bool\n",
        "    reasoning: str\n",
        "\n",
        "# define an agent that checks if user is asking about political opinions\n",
        "politics_agent = Agent(\n",
        "    name=\"Politics check\",\n",
        "    instructions=\"Check if the user is asking you about political opinions\",\n",
        "    output_type=GuardrailOutput,\n",
        ")"
      ],
      "metadata": {
        "id": "I1LT_UpiXFg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can call this agent directly:"
      ],
      "metadata": {
        "id": "vRRFcCFGBHSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what do you think about the labour party in the UK?\"\n",
        "\n",
        "result = await Runner.run(starting_agent=politics_agent, input=query)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4kgYMC9A7WP",
        "outputId": "7660cdce-00f3-41c9-812c-691fd8a85919"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RunResult(input='what do you think about the labour party in the UK?', new_items=[MessageOutputItem(agent=Agent(name='Politics check', instructions='Check if the user is asking you about political opinions', handoff_description=None, handoffs=[], model=None, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None), tools=[], input_guardrails=[], output_guardrails=[], output_type=<class '__main__.GuardrailOutput'>, hooks=None), raw_item=ResponseOutputMessage(id='msg_67d17c686dd081918b1b4094c10715bf0cd98b8cada844ac', content=[ResponseOutputText(annotations=[], text='{\"is_triggered\":true,\"reasoning\":\"The user is asking for an opinion on a political party, which falls under the category of political opinions.\"}', type='output_text')], role='assistant', status='completed', type='message'), type='message_output_item')], raw_responses=[ModelResponse(output=[ResponseOutputMessage(id='msg_67d17c686dd081918b1b4094c10715bf0cd98b8cada844ac', content=[ResponseOutputText(annotations=[], text='{\"is_triggered\":true,\"reasoning\":\"The user is asking for an opinion on a political party, which falls under the category of political opinions.\"}', type='output_text')], role='assistant', status='completed', type='message')], usage=Usage(requests=1, input_tokens=101, output_tokens=33, total_tokens=134), referenceable_id='resp_67d17c67d62c8191a7878d8ecd0693820cd98b8cada844ac')], final_output=GuardrailOutput(is_triggered=True, reasoning='The user is asking for an opinion on a political party, which falls under the category of political opinions.'), input_guardrail_results=[], output_guardrail_results=[], _last_agent=Agent(name='Politics check', instructions='Check if the user is asking you about political opinions', handoff_description=None, handoffs=[], model=None, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None), tools=[], input_guardrails=[], output_guardrails=[], output_type=<class '__main__.GuardrailOutput'>, hooks=None))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output from our agent is hidden away in there, we extract it like so:"
      ],
      "metadata": {
        "id": "6NfFXKYOBeJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result.final_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrS4dT5mC7jN",
        "outputId": "6e464d4f-ecbb-4aed-c0d4-208fcfcc9f17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GuardrailOutput(is_triggered=True, reasoning='The user is asking for an opinion on a political party, which falls under the category of political opinions.')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To integrate this with our other agents we need to move our logic into a single function decorated with the `@input_guardrail` decorator.\n",
        "\n",
        "When defining these guardrails we need to follow the following structure:\n",
        "\n",
        "* Input parameters must include a `ctx` (context), `agent`, and `input` (the user's query in this case). Note that below we will only use the `input` parameter.\n",
        "* Output must be a `GuardrailFunctionOutput` object."
      ],
      "metadata": {
        "id": "okpF-4pAC_zC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import (\n",
        "    GuardrailFunctionOutput,\n",
        "    RunContextWrapper,\n",
        "    input_guardrail\n",
        ")\n",
        "\n",
        "# this is the guardrail function that returns GuardrailFunctionOutput object\n",
        "@input_guardrail\n",
        "async def politics_guardrail(\n",
        "    ctx: RunContextWrapper[None],\n",
        "    agent: Agent,\n",
        "    input: str,\n",
        ") -> GuardrailFunctionOutput:\n",
        "    # run agent to check if guardrail is triggered\n",
        "    response = await Runner.run(starting_agent=politics_agent, input=input)\n",
        "    # format response into GuardrailFunctionOutput\n",
        "    return GuardrailFunctionOutput(\n",
        "        output_info=response.final_output,\n",
        "        tripwire_triggered=response.final_output.is_triggered,\n",
        "    )"
      ],
      "metadata": {
        "id": "o3UQIt-qBLTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can initialize our normal agent with the `input_guardrails` parameter:"
      ],
      "metadata": {
        "id": "N8eCZluTFnYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=(\n",
        "        \"You're a helpful assistant, remember to always \"\n",
        "        \"use the provided tools whenever possible. Do not \"\n",
        "        \"rely on your own knowledge too much and instead \"\n",
        "        \"use your tools to help you answer queries.\"\n",
        "    ),\n",
        "    model=\"gpt-4o-mini\",\n",
        "    tools=[multiply],\n",
        "    input_guardrails=[politics_guardrail],  # note this is a list of guardrails\n",
        ")"
      ],
      "metadata": {
        "id": "_JYELjOSFoED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's run it! We'll stick with `Runner.run` for the sake of brevity:"
      ],
      "metadata": {
        "id": "oxYq9Aq0GIv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = await Runner.run(\n",
        "    starting_agent=agent,\n",
        "    input=\"what is 7.814 multiplied by 103.892?\"\n",
        ")\n",
        "result.final_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GKDUcBYMGGcG",
        "outputId": "e512f6d0-fabb-44a0-c5ce-7a591d2824e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'7.814 multiplied by 103.892 is approximately 811.812.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see if our guardrail will trigger:"
      ],
      "metadata": {
        "id": "c5sN23B_HcPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = await Runner.run(\n",
        "    starting_agent=agent,\n",
        "    input=\"what do you think about the labour party in the UK?\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "-tncyfxYGdRn",
        "outputId": "c35a415e-0d41-487d-9753-cf0151350039"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InputGuardrailTripwireTriggered",
          "evalue": "Guardrail InputGuardrail triggered tripwire",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInputGuardrailTripwireTriggered\u001b[0m           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-93df9a2c5ef4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m result = await Runner.run(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mstarting_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"what do you think about the labour party in the UK?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/agents/run.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(cls, starting_agent, input, context, max_turns, hooks, run_config)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mcurrent_turn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m                         input_guardrail_results, turn_result = await asyncio.gather(\n\u001b[0m\u001b[1;32m    211\u001b[0m                             cls._run_input_guardrails(\n\u001b[1;32m    212\u001b[0m                                 \u001b[0mstarting_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/agents/run.py\u001b[0m in \u001b[0;36m_run_input_guardrails\u001b[0;34m(cls, agent, guardrails, input, context)\u001b[0m\n\u001b[1;32m    803\u001b[0m                     )\n\u001b[1;32m    804\u001b[0m                 )\n\u001b[0;32m--> 805\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mInputGuardrailTripwireTriggered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m                 \u001b[0mguardrail_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInputGuardrailTripwireTriggered\u001b[0m: Guardrail InputGuardrail triggered tripwire"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great, our guardrail triggered! The `output_guardrail` type is implemented in almost the exact same way, but uses the `@output_guardrail` decorator when defining the guardrail function, and the `output_guardrails` parameter when defining our `Agent`."
      ],
      "metadata": {
        "id": "lvGzaTDJHmts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conversational Agents"
      ],
      "metadata": {
        "id": "nnVc_II-IcRg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far we've only seen how to use our agents with single messages. Many use-cases require chat history to make our agents conversational. To implement that we simply provide a list of messages to our `Runner`.\n",
        "\n",
        "Let's see how this works, first we send a single message:"
      ],
      "metadata": {
        "id": "2EV4EtGZIevd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = await Runner.run(\n",
        "    starting_agent=agent,\n",
        "    input=\"remember the number 7.814 for me please\"\n",
        ")\n",
        "result.final_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "wK558-qII5WA",
        "outputId": "5eaf5b88-18ef-4d4b-82ac-bf8511d519f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I can't store or remember information for future use. However, you can save it in a note or use a reminder app. If you have any questions or need assistance with something else, feel free to ask!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fortunately, we can help our agent remember this information. We can use the `.to_input_list()` method to format our `result` into a list of messages for our next query."
      ],
      "metadata": {
        "id": "fD_kDKxFI5LO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result.to_input_list()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xs-buAAvJh8F",
        "outputId": "3b889236-2a26-4c91-bcca-45733518241d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'content': 'remember the number 7.814 for me please', 'role': 'user'},\n",
              " {'id': 'msg_67d17d702a708191ae8704bdade87f7803f330fa9bc6d689',\n",
              "  'content': [{'annotations': [],\n",
              "    'text': \"I can't store or remember information for future use. However, you can save it in a note or use a reminder app. If you have any questions or need assistance with something else, feel free to ask!\",\n",
              "    'type': 'output_text'}],\n",
              "  'role': 'assistant',\n",
              "  'status': 'completed',\n",
              "  'type': 'message'}]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We merge this with our next message:"
      ],
      "metadata": {
        "id": "Ia1n7UG_JkYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = await Runner.run(\n",
        "    starting_agent=agent,\n",
        "    input=result.to_input_list() + [\n",
        "        {\"role\": \"user\", \"content\": \"multiply the last number by 103.892\"}\n",
        "    ]\n",
        ")\n",
        "result.final_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "y4WKSTObJokR",
        "outputId": "01331fad-5e8c-4e64-86cd-a3470432d462"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The result of multiplying 7.814 by 103.892 is approximately 811.812.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3kZHZne6WbkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like our agent can remember our previous interactions after all!"
      ],
      "metadata": {
        "id": "7bGdZMCLKDMK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "khGNoTd_KH4k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "That is our rapid-fire overview of OpenAI's new Agents SDK. We've covered most of the essentials here but there are many other features in the library, and many of the features we included here come with plenty of different ways to use. The SDK is already fairly substantial and certainly worth keeping an eye on."
      ],
      "metadata": {
        "id": "aFi__V9hKIKQ"
      }
    }
  ]
}