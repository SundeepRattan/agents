{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aurelio-labs/cookbook/blob/main/gen-ai/openai/agents-sdk-intro.ipynb)\n"
      ],
      "metadata": {
        "id": "d1DGYFfoKvsB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI's Agents SDK"
      ],
      "metadata": {
        "id": "sG6pIZ-_7uRX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenAI have released an **Agents SDK**, their version of an open source agent development library.\n",
        "\n",
        "OpenAI have outlined a few features of the library:\n",
        "\n",
        "```\n",
        "* Agent loop: Built-in agent loop that handles calling tools, sending results to the LLM, and looping until the LLM is done.\n",
        "* Python-first: Use built-in language features to orchestrate and chain agents, rather than needing to learn new abstractions.\n",
        "* Handoffs: A powerful feature to coordinate and delegate between multiple agents.\n",
        "* Guardrails: Run input validations and checks in parallel to your agents, breaking early if the checks fail.\n",
        "* Function tools: Turn any Python function into a tool, with automatic schema generation and Pydantic-powered validation.\n",
        "* Tracing: Built-in tracing that lets you visualize, debug and monitor your workflows, as well as use the OpenAI suite of evaluation, fine-tuning and distillation tools.\n",
        "```\n",
        "\n",
        "([source](https://openai.github.io/openai-agents-python/))\n",
        "\n",
        "We'll focus on covering the essentials here - including the **agent loop**, **python-first**, **guardrails**, and **function tools** features.\n",
        "\n",
        "Let's start by installing the library:"
      ],
      "metadata": {
        "id": "rN2gNfry7tDp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3aqSlMc27noo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f5bb325-a199-42b6-a945-e7c25a3959b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/75.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m71.7/75.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.5/75.5 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU openai-agents==0.0.3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let's set our [OpenAI API key](https://platform.openai.com/settings/organization/api-keys)."
      ],
      "metadata": {
        "id": "nBGHkh7nBVHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or \\\n",
        "  getpass(\"Enter your OpenAI API key: \")"
      ],
      "metadata": {
        "id": "Kwpk_H5_BnAl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f428812f-7f4a-4313-bae3-cb6254eb54b1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Agent, Runner\n",
        "\n",
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=\"You're a helpful assistant\",\n",
        "    model=\"gpt-4.1-nano\",\n",
        ")"
      ],
      "metadata": {
        "id": "lqkaleTAAjHK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running our Agent"
      ],
      "metadata": {
        "id": "0-zOBFhAEROm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenAI gives us three methods for running our agent, all via a `Runner` class — those methods are:\n",
        "\n",
        "1. `Runner.run()` which runs in async.\n",
        "2. `Runner.run_sync()` which runs in sync.\n",
        "3. `Runner.run_streamed()` which runs in async _and_ streams the response back to us.\n",
        "\n",
        "We'll quicky test method **(1)**:"
      ],
      "metadata": {
        "id": "dWO4ozC8ETNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = await Runner.run(\n",
        "    starting_agent=agent,\n",
        "    input=\"tell me a short story\"\n",
        ")\n",
        "result.final_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "Bjn-gyR9BIn-",
        "outputId": "fc80e2e1-1967-4f2e-f5d8-9e90262ecdc4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Once upon a time in a quiet village, there was a curious kitten named Luna. Luna loved exploring every nook and cranny of her tiny world, especially the old oak tree in the village square. One day, she decided to climb to the very top, where she discovered a shimmering, hidden nest filled with colorful eggs. As she gently pawed at them, a gentle voice spoke. It was the wise old owl living in the tree, who told Luna that the eggs were magical and brought good luck. From that day on, Luna visited the oak tree often, sharing her adventures and spreading joy throughout the village. And everyone agreed, Luna's curiosity made her the happiest kitten of all.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In most scenarios we'll likely want to be using method **(3)**, ie running async and streaming tokens. To do this we need to write a little more code to handle the async streaming and print the tokens as they're returned.\n",
        "\n",
        "First, we create a `RunResultStreaming` object by calling `Runner.run_streamed(...)`, we then _asynchronously_ iterate through the streamed events returned by our LLM using the `response.stream_events()` method:"
      ],
      "metadata": {
        "id": "W5_yue8fE4eX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = Runner.run_streamed(\n",
        "    starting_agent=agent,\n",
        "    input=\"hello there\"\n",
        ")\n",
        "async for event in response.stream_events():\n",
        "    print(event)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBnhUepDBPHM",
        "outputId": "29dc2285-b539-4458-f329-2563d55ea4a5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AgentUpdatedStreamEvent(new_agent=Agent(name='Assistant', instructions=\"You're a helpful assistant\", handoff_description=None, handoffs=[], model='gpt-4.1-nano', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None), tools=[], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None), type='agent_updated_stream_event')\n",
            "RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='resp_6824fbcf71708198b641429ae752228604e9eff182c0dbf9', created_at=1747254223.0, error=None, incomplete_details=None, instructions=\"You're a helpful assistant\", metadata={}, model='gpt-4.1-nano-2025-04-14', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='auto', status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=None, user=None, store=True), type='response.created'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseInProgressEvent(response=Response(id='resp_6824fbcf71708198b641429ae752228604e9eff182c0dbf9', created_at=1747254223.0, error=None, incomplete_details=None, instructions=\"You're a helpful assistant\", metadata={}, model='gpt-4.1-nano-2025-04-14', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='auto', status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=None, user=None, store=True), type='response.in_progress'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='msg_6824fbcfb3808198878d55c241a30c2604e9eff182c0dbf9', content=[], role='assistant', status='in_progress', type='message'), output_index=0, type='response.output_item.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseContentPartAddedEvent(content_index=0, item_id='msg_6824fbcfb3808198878d55c241a30c2604e9eff182c0dbf9', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text'), type='response.content_part.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='Hello', item_id='msg_6824fbcfb3808198878d55c241a30c2604e9eff182c0dbf9', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='!', item_id='msg_6824fbcfb3808198878d55c241a30c2604e9eff182c0dbf9', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' How', item_id='msg_6824fbcfb3808198878d55c241a30c2604e9eff182c0dbf9', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' can', item_id='msg_6824fbcfb3808198878d55c241a30c2604e9eff182c0dbf9', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' I', item_id='msg_6824fbcfb3808198878d55c241a30c2604e9eff182c0dbf9', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' assist', item_id='msg_6824fbcfb3808198878d55c241a30c2604e9eff182c0dbf9', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' you', item_id='msg_6824fbcfb3808198878d55c241a30c2604e9eff182c0dbf9', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' today', item_id='msg_6824fbcfb3808198878d55c241a30c2604e9eff182c0dbf9', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='?', item_id='msg_6824fbcfb3808198878d55c241a30c2604e9eff182c0dbf9', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDoneEvent(content_index=0, item_id='msg_6824fbcfb3808198878d55c241a30c2604e9eff182c0dbf9', output_index=0, text='Hello! How can I assist you today?', type='response.output_text.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseContentPartDoneEvent(content_index=0, item_id='msg_6824fbcfb3808198878d55c241a30c2604e9eff182c0dbf9', output_index=0, part=ResponseOutputText(annotations=[], text='Hello! How can I assist you today?', type='output_text'), type='response.content_part.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='msg_6824fbcfb3808198878d55c241a30c2604e9eff182c0dbf9', content=[ResponseOutputText(annotations=[], text='Hello! How can I assist you today?', type='output_text')], role='assistant', status='completed', type='message'), output_index=0, type='response.output_item.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='resp_6824fbcf71708198b641429ae752228604e9eff182c0dbf9', created_at=1747254223.0, error=None, incomplete_details=None, instructions=\"You're a helpful assistant\", metadata={}, model='gpt-4.1-nano-2025-04-14', object='response', output=[ResponseOutputMessage(id='msg_6824fbcfb3808198878d55c241a30c2604e9eff182c0dbf9', content=[ResponseOutputText(annotations=[], text='Hello! How can I assist you today?', type='output_text')], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=ResponseUsage(input_tokens=17, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=10, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=27), user=None, store=True), type='response.completed'), type='raw_response_event')\n",
            "RunItemStreamEvent(name='message_output_created', item=MessageOutputItem(agent=Agent(name='Assistant', instructions=\"You're a helpful assistant\", handoff_description=None, handoffs=[], model='gpt-4.1-nano', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None), tools=[], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None), raw_item=ResponseOutputMessage(id='msg_6824fbcfb3808198878d55c241a30c2604e9eff182c0dbf9', content=[ResponseOutputText(annotations=[], text='Hello! How can I assist you today?', type='output_text')], role='assistant', status='completed', type='message'), type='message_output_item'), type='run_item_stream_event')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can filter these various event types to find only raw tokens like so:"
      ],
      "metadata": {
        "id": "iH-RoQfzMj6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai.types.responses import ResponseTextDeltaEvent\n",
        "\n",
        "# we do need to reinitialize our runner before re-executing\n",
        "response = Runner.run_streamed(\n",
        "    starting_agent=agent,\n",
        "    input=\"tell me a cool story\"\n",
        ")\n",
        "\n",
        "async for event in response.stream_events():\n",
        "    if event.type == \"raw_response_event\" and \\\n",
        "        isinstance(event.data, ResponseTextDeltaEvent):\n",
        "        print(event.data.delta, end=\"\", flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGQANyvXG9zJ",
        "outputId": "7e86f371-4bc7-4256-8e0d-76879c951aa5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time, in a hidden corner of the world, there was a small village called Lumina where the night sky shimmered brighter than anywhere else. The villagers believed that the stars were actually tiny enchanted lanterns dropped by celestial beings, guiding lost souls and inspiring dreams.\n",
            "\n",
            "Among these villagers was a curious young girl named Mira. She often wandered into the woods, captivated by the glow of the stars and the whispering winds. One evening, as she sat beneath her favorite oak tree, she noticed a peculiar flickering light deep within the forest. Drawn by curiosity, she decided to follow it.\n",
            "\n",
            "The light led her to a secret clearing she’d never seen before. In the center stood a shimmering crystal pedestal, cradling a glowing, floating star. The star pulsed with a gentle, warm light. Mira reached out, and as her fingers touched the star, she felt a surge of warmth and knowledge flow through her.\n",
            "\n",
            "Suddenly, the sky above lit up brighter, and she realized that she had been chosen as the guardian of the celestial lanterns. The star whispered to her to keep the balance between dreams and reality, guiding those with hope and courage.\n",
            "\n",
            "From that day on, Mira became the protector of Lumina's celestial secrets. Each night, she watched over the stars, ensuring that their magic continued to inspire the smallest dreams and the greatest adventures. Her story became a legend, reminding everyone that sometimes, the brightest lights come from within when we dare to follow our curiosity and believe in the magic of the universe.\n",
            "\n",
            "And so, under the eternal glow of the enchanted night sky, Lumina thrived, a place where dreams never faded and the stars always watched over those brave enough to seek their own destiny.\n",
            "\n",
            "The end."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tools"
      ],
      "metadata": {
        "id": "9Iixlv0sRMqr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenAI included **function tools** as a key feature in their Agents SDK announcement. After turning everyone away from using _function calling_ to instead use _tool calling_, OpenAI have now decided that an LLM deciding to execute some code will be called _\"function tools\"_.\n",
        "\n",
        "To use _function tools_ in Agents SDK we simply decorate a function with the `@function_tool` decorator like so:"
      ],
      "metadata": {
        "id": "kDHORYUcROZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import function_tool\n",
        "\n",
        "@function_tool\n",
        "def multiply(x: float, y: float) -> float:\n",
        "    \"\"\"Multiplies `x` and `y` to provide a precise\n",
        "    answer.\"\"\"\n",
        "    return x*y"
      ],
      "metadata": {
        "id": "tJz7BlwiNOa3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we have taken extra care to include a clear and descriptive function name, relatively clear parameter names, type annotations for both input parameters and expected output, and a natural language docstring that will be fed to the LLM and explain to it _what_ this tool does.\n",
        "\n",
        "To run our agent _with_ tools we simply pass our new tool into the `tools` parameter during `Agent` initialization."
      ],
      "metadata": {
        "id": "I8b8KektTQbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent_multiply = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=(\n",
        "        \"You're a helpful assistant, remember to always \"\n",
        "        \"use the provided tools whenever possible. Do not \"\n",
        "        \"rely on your own knowledge too much and instead \"\n",
        "        \"use your tools to help you answer queries.\"\n",
        "    ),\n",
        "    model=\"gpt-4.1-nano\",\n",
        "    tools=[multiply]  # note that we expect a list of tools\n",
        ")"
      ],
      "metadata": {
        "id": "WnT4C9WBTO_M"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's initialize a new runner and execute our agent with tools:"
      ],
      "metadata": {
        "id": "VtioNxflUS-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = Runner.run_streamed(\n",
        "    starting_agent=agent_multiply,\n",
        "    input=\"what is 7.814 multiplied by 103.892?\"\n",
        ")\n",
        "\n",
        "async for event in response.stream_events():\n",
        "    print(event)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSOREQ-XUSVi",
        "outputId": "b77b1ba7-6294-416d-94e0-ebc3ead812c2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AgentUpdatedStreamEvent(new_agent=Agent(name='Assistant', instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", handoff_description=None, handoffs=[], model='gpt-4.1-nano', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None), tools=[FunctionTool(name='multiply', description='Multiplies `x` and `y` to provide a precise\\nanswer.', params_json_schema={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7d9954c359e0>, strict_json_schema=True)], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None), type='agent_updated_stream_event')\n",
            "RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='resp_6824fd9241148198a7974c8cb175bb100c41536071889b3e', created_at=1747254674.0, error=None, incomplete_details=None, instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", metadata={}, model='gpt-4.1-nano-2025-04-14', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='multiply', parameters={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, strict=True, type='function', description='Multiplies `x` and `y` to provide a precise\\nanswer.')], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='auto', status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=None, user=None, store=True), type='response.created'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseInProgressEvent(response=Response(id='resp_6824fd9241148198a7974c8cb175bb100c41536071889b3e', created_at=1747254674.0, error=None, incomplete_details=None, instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", metadata={}, model='gpt-4.1-nano-2025-04-14', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='multiply', parameters={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, strict=True, type='function', description='Multiplies `x` and `y` to provide a precise\\nanswer.')], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='auto', status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=None, user=None, store=True), type='response.in_progress'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseFunctionToolCall(arguments='', call_id='call_Z58izjoepkVnEey8s9vOOwSp', name='multiply', type='function_call', id='fc_6824fd92ac7881988f2618765be5d84e0c41536071889b3e', status='in_progress'), output_index=0, type='response.output_item.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='{', item_id='fc_6824fd92ac7881988f2618765be5d84e0c41536071889b3e', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='\"x', item_id='fc_6824fd92ac7881988f2618765be5d84e0c41536071889b3e', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='\":', item_id='fc_6824fd92ac7881988f2618765be5d84e0c41536071889b3e', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='7.814,', item_id='fc_6824fd92ac7881988f2618765be5d84e0c41536071889b3e', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='\"y', item_id='fc_6824fd92ac7881988f2618765be5d84e0c41536071889b3e', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='\":', item_id='fc_6824fd92ac7881988f2618765be5d84e0c41536071889b3e', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='103.892}', item_id='fc_6824fd92ac7881988f2618765be5d84e0c41536071889b3e', output_index=0, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDoneEvent(arguments='{\"x\":7.814,\"y\":103.892}', item_id='fc_6824fd92ac7881988f2618765be5d84e0c41536071889b3e', output_index=0, type='response.function_call_arguments.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseFunctionToolCall(arguments='{\"x\":7.814,\"y\":103.892}', call_id='call_Z58izjoepkVnEey8s9vOOwSp', name='multiply', type='function_call', id='fc_6824fd92ac7881988f2618765be5d84e0c41536071889b3e', status='completed'), output_index=0, type='response.output_item.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseFunctionToolCall(arguments='', call_id='call_kXVYx6BFVn9ESLsexVkv14dW', name='multiply', type='function_call', id='fc_6824fd92c56481989939176814d58d3e0c41536071889b3e', status='in_progress'), output_index=1, type='response.output_item.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='{', item_id='fc_6824fd92c56481989939176814d58d3e0c41536071889b3e', output_index=1, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='\"x', item_id='fc_6824fd92c56481989939176814d58d3e0c41536071889b3e', output_index=1, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='\":', item_id='fc_6824fd92c56481989939176814d58d3e0c41536071889b3e', output_index=1, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='7.814,', item_id='fc_6824fd92c56481989939176814d58d3e0c41536071889b3e', output_index=1, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='\"y', item_id='fc_6824fd92c56481989939176814d58d3e0c41536071889b3e', output_index=1, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='\":', item_id='fc_6824fd92c56481989939176814d58d3e0c41536071889b3e', output_index=1, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='103.892}', item_id='fc_6824fd92c56481989939176814d58d3e0c41536071889b3e', output_index=1, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDoneEvent(arguments='{\"x\":7.814,\"y\":103.892}', item_id='fc_6824fd92c56481989939176814d58d3e0c41536071889b3e', output_index=1, type='response.function_call_arguments.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseFunctionToolCall(arguments='{\"x\":7.814,\"y\":103.892}', call_id='call_kXVYx6BFVn9ESLsexVkv14dW', name='multiply', type='function_call', id='fc_6824fd92c56481989939176814d58d3e0c41536071889b3e', status='completed'), output_index=1, type='response.output_item.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='resp_6824fd9241148198a7974c8cb175bb100c41536071889b3e', created_at=1747254674.0, error=None, incomplete_details=None, instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", metadata={}, model='gpt-4.1-nano-2025-04-14', object='response', output=[ResponseFunctionToolCall(arguments='{\"x\":7.814,\"y\":103.892}', call_id='call_Z58izjoepkVnEey8s9vOOwSp', name='multiply', type='function_call', id='fc_6824fd92ac7881988f2618765be5d84e0c41536071889b3e', status='completed'), ResponseFunctionToolCall(arguments='{\"x\":7.814,\"y\":103.892}', call_id='call_kXVYx6BFVn9ESLsexVkv14dW', name='multiply', type='function_call', id='fc_6824fd92c56481989939176814d58d3e0c41536071889b3e', status='completed')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='multiply', parameters={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, strict=True, type='function', description='Multiplies `x` and `y` to provide a precise\\nanswer.')], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=ResponseUsage(input_tokens=0, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=0, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=0), user=None, store=True), type='response.completed'), type='raw_response_event')\n",
            "RunItemStreamEvent(name='tool_called', item=ToolCallItem(agent=Agent(name='Assistant', instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", handoff_description=None, handoffs=[], model='gpt-4.1-nano', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None), tools=[FunctionTool(name='multiply', description='Multiplies `x` and `y` to provide a precise\\nanswer.', params_json_schema={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7d9954c359e0>, strict_json_schema=True)], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None), raw_item=ResponseFunctionToolCall(arguments='{\"x\":7.814,\"y\":103.892}', call_id='call_Z58izjoepkVnEey8s9vOOwSp', name='multiply', type='function_call', id='fc_6824fd92ac7881988f2618765be5d84e0c41536071889b3e', status='completed'), type='tool_call_item'), type='run_item_stream_event')\n",
            "RunItemStreamEvent(name='tool_called', item=ToolCallItem(agent=Agent(name='Assistant', instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", handoff_description=None, handoffs=[], model='gpt-4.1-nano', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None), tools=[FunctionTool(name='multiply', description='Multiplies `x` and `y` to provide a precise\\nanswer.', params_json_schema={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7d9954c359e0>, strict_json_schema=True)], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None), raw_item=ResponseFunctionToolCall(arguments='{\"x\":7.814,\"y\":103.892}', call_id='call_kXVYx6BFVn9ESLsexVkv14dW', name='multiply', type='function_call', id='fc_6824fd92c56481989939176814d58d3e0c41536071889b3e', status='completed'), type='tool_call_item'), type='run_item_stream_event')\n",
            "RunItemStreamEvent(name='tool_output', item=ToolCallOutputItem(agent=Agent(name='Assistant', instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", handoff_description=None, handoffs=[], model='gpt-4.1-nano', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None), tools=[FunctionTool(name='multiply', description='Multiplies `x` and `y` to provide a precise\\nanswer.', params_json_schema={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7d9954c359e0>, strict_json_schema=True)], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None), raw_item={'call_id': 'call_Z58izjoepkVnEey8s9vOOwSp', 'output': '811.812088', 'type': 'function_call_output'}, output='811.812088', type='tool_call_output_item'), type='run_item_stream_event')\n",
            "RunItemStreamEvent(name='tool_output', item=ToolCallOutputItem(agent=Agent(name='Assistant', instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", handoff_description=None, handoffs=[], model='gpt-4.1-nano', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None), tools=[FunctionTool(name='multiply', description='Multiplies `x` and `y` to provide a precise\\nanswer.', params_json_schema={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7d9954c359e0>, strict_json_schema=True)], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None), raw_item={'call_id': 'call_kXVYx6BFVn9ESLsexVkv14dW', 'output': '811.812088', 'type': 'function_call_output'}, output='811.812088', type='tool_call_output_item'), type='run_item_stream_event')\n",
            "RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='resp_6824fd93055881989f22ddb1212718640c41536071889b3e', created_at=1747254675.0, error=None, incomplete_details=None, instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", metadata={}, model='gpt-4.1-nano-2025-04-14', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='multiply', parameters={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, strict=True, type='function', description='Multiplies `x` and `y` to provide a precise\\nanswer.')], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='auto', status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=None, user=None, store=True), type='response.created'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseInProgressEvent(response=Response(id='resp_6824fd93055881989f22ddb1212718640c41536071889b3e', created_at=1747254675.0, error=None, incomplete_details=None, instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", metadata={}, model='gpt-4.1-nano-2025-04-14', object='response', output=[], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='multiply', parameters={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, strict=True, type='function', description='Multiplies `x` and `y` to provide a precise\\nanswer.')], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='auto', status='in_progress', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=None, user=None, store=True), type='response.in_progress'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='msg_6824fd935f88819893ecfd6bb303ded60c41536071889b3e', content=[], role='assistant', status='in_progress', type='message'), output_index=0, type='response.output_item.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseContentPartAddedEvent(content_index=0, item_id='msg_6824fd935f88819893ecfd6bb303ded60c41536071889b3e', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text'), type='response.content_part.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='The', item_id='msg_6824fd935f88819893ecfd6bb303ded60c41536071889b3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' result', item_id='msg_6824fd935f88819893ecfd6bb303ded60c41536071889b3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' of', item_id='msg_6824fd935f88819893ecfd6bb303ded60c41536071889b3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' multiplying', item_id='msg_6824fd935f88819893ecfd6bb303ded60c41536071889b3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' ', item_id='msg_6824fd935f88819893ecfd6bb303ded60c41536071889b3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='7', item_id='msg_6824fd935f88819893ecfd6bb303ded60c41536071889b3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='.', item_id='msg_6824fd935f88819893ecfd6bb303ded60c41536071889b3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='814', item_id='msg_6824fd935f88819893ecfd6bb303ded60c41536071889b3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' by', item_id='msg_6824fd935f88819893ecfd6bb303ded60c41536071889b3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' ', item_id='msg_6824fd935f88819893ecfd6bb303ded60c41536071889b3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='103', item_id='msg_6824fd935f88819893ecfd6bb303ded60c41536071889b3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='.', item_id='msg_6824fd935f88819893ecfd6bb303ded60c41536071889b3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='892', item_id='msg_6824fd935f88819893ecfd6bb303ded60c41536071889b3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' is', item_id='msg_6824fd935f88819893ecfd6bb303ded60c41536071889b3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' approximately', item_id='msg_6824fd935f88819893ecfd6bb303ded60c41536071889b3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' ', item_id='msg_6824fd935f88819893ecfd6bb303ded60c41536071889b3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='811', item_id='msg_6824fd935f88819893ecfd6bb303ded60c41536071889b3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='.', item_id='msg_6824fd935f88819893ecfd6bb303ded60c41536071889b3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='81', item_id='msg_6824fd935f88819893ecfd6bb303ded60c41536071889b3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='.', item_id='msg_6824fd935f88819893ecfd6bb303ded60c41536071889b3e', output_index=0, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDoneEvent(content_index=0, item_id='msg_6824fd935f88819893ecfd6bb303ded60c41536071889b3e', output_index=0, text='The result of multiplying 7.814 by 103.892 is approximately 811.81.', type='response.output_text.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseContentPartDoneEvent(content_index=0, item_id='msg_6824fd935f88819893ecfd6bb303ded60c41536071889b3e', output_index=0, part=ResponseOutputText(annotations=[], text='The result of multiplying 7.814 by 103.892 is approximately 811.81.', type='output_text'), type='response.content_part.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='msg_6824fd935f88819893ecfd6bb303ded60c41536071889b3e', content=[ResponseOutputText(annotations=[], text='The result of multiplying 7.814 by 103.892 is approximately 811.81.', type='output_text')], role='assistant', status='completed', type='message'), output_index=0, type='response.output_item.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='resp_6824fd93055881989f22ddb1212718640c41536071889b3e', created_at=1747254675.0, error=None, incomplete_details=None, instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", metadata={}, model='gpt-4.1-nano-2025-04-14', object='response', output=[ResponseOutputMessage(id='msg_6824fd935f88819893ecfd6bb303ded60c41536071889b3e', content=[ResponseOutputText(annotations=[], text='The result of multiplying 7.814 by 103.892 is approximately 811.81.', type='output_text')], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='multiply', parameters={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, strict=True, type='function', description='Multiplies `x` and `y` to provide a precise\\nanswer.')], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=ResponseUsage(input_tokens=173, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=22, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=195), user=None, store=True), type='response.completed'), type='raw_response_event')\n",
            "RunItemStreamEvent(name='message_output_created', item=MessageOutputItem(agent=Agent(name='Assistant', instructions=\"You're a helpful assistant, remember to always use the provided tools whenever possible. Do not rely on your own knowledge too much and instead use your tools to help you answer queries.\", handoff_description=None, handoffs=[], model='gpt-4.1-nano', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None), tools=[FunctionTool(name='multiply', description='Multiplies `x` and `y` to provide a precise\\nanswer.', params_json_schema={'properties': {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}, 'required': ['x', 'y'], 'title': 'multiply_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7d9954c359e0>, strict_json_schema=True)], input_guardrails=[], output_guardrails=[], output_type=None, hooks=None), raw_item=ResponseOutputMessage(id='msg_6824fd935f88819893ecfd6bb303ded60c41536071889b3e', content=[ResponseOutputText(annotations=[], text='The result of multiplying 7.814 by 103.892 is approximately 811.81.', type='output_text')], role='assistant', status='completed', type='message'), type='message_output_item'), type='run_item_stream_event')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_multiply = Runner.run_streamed(\n",
        "    starting_agent=agent_multiply,\n",
        "    input=\"what is 7.814 multiplied by 103.892?\"\n",
        ")\n",
        "\n",
        "async for event in response_multiply.stream_events():\n",
        "    if event.type == \"raw_response_event\" and \\\n",
        "        isinstance(event.data, ResponseTextDeltaEvent):\n",
        "        print(event.data.delta, end=\"\", flush=True)"
      ],
      "metadata": {
        "id": "N8XKYo6xfwpT",
        "outputId": "fa49d53b-ebbd-4eff-cfd5-332952bacd25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The result of multiplying 7.814 by 103.892 is approximately 811.81."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we look closely at the fourth event object we will see `ResponseFunctionToolCall`, meaning our `multiply` tool was called by our LLM. Following this event object we can also see several events containing the `ResponseFunctionCallArgumentsDeltaEvent` type inside the `data` field — these are the input parameters for our tool.\n",
        "\n",
        "Let's rerun that but this time we will process the event outputs to generate a cleaner and more readable output."
      ],
      "metadata": {
        "id": "jQZu6iyPU0wG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai.types.responses import (\n",
        "    ResponseFunctionCallArgumentsDeltaEvent,  # tool call streaming\n",
        "    ResponseCreatedEvent,  # start of new event like tool call or final answer\n",
        ")\n",
        "\n",
        "response = Runner.run_streamed(\n",
        "    starting_agent=agent,\n",
        "    input=\"what is 7.814 multiplied by 103.892?\"\n",
        ")\n",
        "\n",
        "async for event in response.stream_events():\n",
        "    if event.type == \"raw_response_event\":\n",
        "        if isinstance(event.data, ResponseFunctionCallArgumentsDeltaEvent):\n",
        "            # this is streamed parameters for our tool call\n",
        "            print(event.data.delta, end=\"\", flush=True)\n",
        "        elif isinstance(event.data, ResponseTextDeltaEvent):\n",
        "            # this is streamed final answer tokens\n",
        "            print(event.data.delta, end=\"\", flush=True)\n",
        "    elif event.type == \"agent_updated_stream_event\":\n",
        "        # this tells us which agent is currently in use\n",
        "        print(f\"> Current Agent: {event.new_agent.name}\")\n",
        "    elif event.type == \"run_item_stream_event\":\n",
        "        # these are events containing info that we'd typically\n",
        "        # stream out to a user or some downstream process\n",
        "        if event.name == \"tool_called\":\n",
        "            # this is the collection of our _full_ tool call after our tool\n",
        "            # tokens have all been streamed\n",
        "            print()\n",
        "            print(f\"> Tool Called, name: {event.item.raw_item.name}\")\n",
        "            print(f\"> Tool Called, args: {event.item.raw_item.arguments}\")\n",
        "        elif event.name == \"tool_output\":\n",
        "            # this is the response from our tool execution\n",
        "            print(f\"> Tool Output: {event.item.raw_item['output']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhFvNVowUrd5",
        "outputId": "0e84ba01-fb1f-4c0c-eacd-9a844f06e1a5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Current Agent: Assistant\n",
            "{\"x\":7.814,\"y\":103.892}\n",
            "> Tool Called, name: multiply\n",
            "> Tool Called, args: {\"x\":7.814,\"y\":103.892}\n",
            "> Tool Output: 811.812088\n",
            "7.814 multiplied by 103.892 equals approximately 811.812088."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Guardrails"
      ],
      "metadata": {
        "id": "CqqWET4O93il"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenAI have also included guardrails in the Agents SDK. These come as _input guardrails_ and _output guardrails_, the `input_guardrail` checks that the input going into your LLM is \"safe\" and the `output_guardrail` checks that the output from your LLM is \"safe\".\n",
        "\n",
        "Let's see how to use them. First, we'll implement a guardrail powered by another LLM (more tokens means more $$$ for OpenAI)."
      ],
      "metadata": {
        "id": "CBrf2Ao396Dg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "# define structure of output for any guardrail agents\n",
        "class GuardrailOutput(BaseModel):\n",
        "    is_triggered: bool\n",
        "    reasoning: str\n",
        "\n",
        "# define an agent that checks if user is asking about political opinions\n",
        "politics_agent = Agent(\n",
        "    name=\"Politics check\",\n",
        "    instructions=\"Check if the user is asking you about political opinions\",\n",
        "    output_type=GuardrailOutput,\n",
        ")"
      ],
      "metadata": {
        "id": "I1LT_UpiXFg0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can call this agent directly:"
      ],
      "metadata": {
        "id": "vRRFcCFGBHSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what do you think about the labour party in the UK?\"\n",
        "# query = \"How is the weather today?\"\n",
        "\n",
        "result = await Runner.run(starting_agent=politics_agent, input=query)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4kgYMC9A7WP",
        "outputId": "7ea0decc-c5ef-47e1-ec7e-1d0d52159f1a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RunResult(input='what do you think about the labour party in the UK?', new_items=[MessageOutputItem(agent=Agent(name='Politics check', instructions='Check if the user is asking you about political opinions', handoff_description=None, handoffs=[], model=None, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None), tools=[], input_guardrails=[], output_guardrails=[], output_type=<class '__main__.GuardrailOutput'>, hooks=None), raw_item=ResponseOutputMessage(id='msg_6824ffebdcfc8198bcc3b81e71be5b69032d3868b83e0b04', content=[ResponseOutputText(annotations=[], text='{\"is_triggered\":true,\"reasoning\":\"The user is explicitly asking for an opinion on a political party, the Labour Party in the UK. This involves political opinions and preferences.\"}', type='output_text')], role='assistant', status='completed', type='message'), type='message_output_item')], raw_responses=[ModelResponse(output=[ResponseOutputMessage(id='msg_6824ffebdcfc8198bcc3b81e71be5b69032d3868b83e0b04', content=[ResponseOutputText(annotations=[], text='{\"is_triggered\":true,\"reasoning\":\"The user is explicitly asking for an opinion on a political party, the Labour Party in the UK. This involves political opinions and preferences.\"}', type='output_text')], role='assistant', status='completed', type='message')], usage=Usage(requests=1, input_tokens=83, output_tokens=39, total_tokens=122), referenceable_id='resp_6824ffebab748198bed42c73d26ca439032d3868b83e0b04')], final_output=GuardrailOutput(is_triggered=True, reasoning='The user is explicitly asking for an opinion on a political party, the Labour Party in the UK. This involves political opinions and preferences.'), input_guardrail_results=[], output_guardrail_results=[], _last_agent=Agent(name='Politics check', instructions='Check if the user is asking you about political opinions', handoff_description=None, handoffs=[], model=None, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=False, truncation=None), tools=[], input_guardrails=[], output_guardrails=[], output_type=<class '__main__.GuardrailOutput'>, hooks=None))"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output from our agent is hidden away in there, we extract it like so:"
      ],
      "metadata": {
        "id": "6NfFXKYOBeJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result.final_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrS4dT5mC7jN",
        "outputId": "bbe9cc24-527b-4ac6-81eb-8567fb1c88be"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GuardrailOutput(is_triggered=True, reasoning='The user is explicitly asking for an opinion on a political party, the Labour Party in the UK. This involves political opinions and preferences.')"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To integrate this with our other agents we need to move our logic into a single function decorated with the `@input_guardrail` decorator.\n",
        "\n",
        "When defining these guardrails we need to follow the following structure:\n",
        "\n",
        "* Input parameters must include a `ctx` (context), `agent`, and `input` (the user's query in this case). Note that below we will only use the `input` parameter.\n",
        "* Output must be a `GuardrailFunctionOutput` object."
      ],
      "metadata": {
        "id": "okpF-4pAC_zC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import (\n",
        "    GuardrailFunctionOutput,\n",
        "    RunContextWrapper,\n",
        "    input_guardrail\n",
        ")\n",
        "\n",
        "# this is the guardrail function that returns GuardrailFunctionOutput object\n",
        "@input_guardrail\n",
        "async def politics_guardrail(\n",
        "    ctx: RunContextWrapper[None],\n",
        "    agent: Agent,\n",
        "    input: str,\n",
        ") -> GuardrailFunctionOutput:\n",
        "    # run agent to check if guardrail is triggered\n",
        "    response = await Runner.run(starting_agent=politics_agent, input=input)\n",
        "    # format response into GuardrailFunctionOutput\n",
        "    return GuardrailFunctionOutput(\n",
        "        output_info=response.final_output,\n",
        "        tripwire_triggered=response.final_output.is_triggered,\n",
        "    )"
      ],
      "metadata": {
        "id": "o3UQIt-qBLTK"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can initialize our normal agent with the `input_guardrails` parameter:"
      ],
      "metadata": {
        "id": "N8eCZluTFnYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(\n",
        "    name=\"Assistant\",\n",
        "    instructions=(\n",
        "        \"You're a helpful assistant, remember to always \"\n",
        "        \"use the provided tools whenever possible. Do not \"\n",
        "        \"rely on your own knowledge too much and instead \"\n",
        "        \"use your tools to help you answer queries.\"\n",
        "    ),\n",
        "    model=\"gpt-4.1-nano\",\n",
        "    tools=[multiply],\n",
        "    input_guardrails=[politics_guardrail],  # note this is a list of guardrails\n",
        ")"
      ],
      "metadata": {
        "id": "_JYELjOSFoED"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's run it! We'll stick with `Runner.run` for the sake of brevity:"
      ],
      "metadata": {
        "id": "oxYq9Aq0GIv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = await Runner.run(\n",
        "    starting_agent=agent,\n",
        "    input=\"what is 7.814 multiplied by 103.892?\"\n",
        ")\n",
        "result.final_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GKDUcBYMGGcG",
        "outputId": "77b01ba6-ed07-4c93-aebd-67aeb08da1f1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The result of multiplying 7.814 by 103.892 is approximately 811.81.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see if our guardrail will trigger:"
      ],
      "metadata": {
        "id": "c5sN23B_HcPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = await Runner.run(\n",
        "    starting_agent=agent,\n",
        "    input=\"what do you think about the labour party in the UK?\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "-tncyfxYGdRn",
        "outputId": "a59f9647-7fa6-4419-f197-5d9c6577892d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InputGuardrailTripwireTriggered",
          "evalue": "Guardrail InputGuardrail triggered tripwire",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInputGuardrailTripwireTriggered\u001b[0m           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-93df9a2c5ef4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m result = await Runner.run(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mstarting_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"what do you think about the labour party in the UK?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/agents/run.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(cls, starting_agent, input, context, max_turns, hooks, run_config)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mcurrent_turn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m                         input_guardrail_results, turn_result = await asyncio.gather(\n\u001b[0m\u001b[1;32m    211\u001b[0m                             cls._run_input_guardrails(\n\u001b[1;32m    212\u001b[0m                                 \u001b[0mstarting_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/agents/run.py\u001b[0m in \u001b[0;36m_run_input_guardrails\u001b[0;34m(cls, agent, guardrails, input, context)\u001b[0m\n\u001b[1;32m    803\u001b[0m                     )\n\u001b[1;32m    804\u001b[0m                 )\n\u001b[0;32m--> 805\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mInputGuardrailTripwireTriggered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m                 \u001b[0mguardrail_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInputGuardrailTripwireTriggered\u001b[0m: Guardrail InputGuardrail triggered tripwire"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great, our guardrail triggered! The `output_guardrail` type is implemented in almost the exact same way, but uses the `@output_guardrail` decorator when defining the guardrail function, and the `output_guardrails` parameter when defining our `Agent`."
      ],
      "metadata": {
        "id": "lvGzaTDJHmts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conversational Agents"
      ],
      "metadata": {
        "id": "nnVc_II-IcRg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far we've only seen how to use our agents with single messages. Many use-cases require chat history to make our agents conversational. To implement that we simply provide a list of messages to our `Runner`.\n",
        "\n",
        "Let's see how this works, first we send a single message:"
      ],
      "metadata": {
        "id": "2EV4EtGZIevd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = await Runner.run(\n",
        "    starting_agent=agent,\n",
        "    input=\"remember the number 7.814 for me please\"\n",
        ")\n",
        "result.final_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wK558-qII5WA",
        "outputId": "16f82744-0d10-4b5e-da94-6dcc92baf858"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I have stored the number 7.814 for you.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fortunately, we can help our agent remember this information. We can use the `.to_input_list()` method to format our `result` into a list of messages for our next query."
      ],
      "metadata": {
        "id": "fD_kDKxFI5LO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result.to_input_list()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xs-buAAvJh8F",
        "outputId": "0640a70c-b532-478d-8a18-d6c6ec29f722"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'content': 'remember the number 7.814 for me please', 'role': 'user'},\n",
              " {'arguments': '{\"x\":7.814,\"y\":1}',\n",
              "  'call_id': 'call_G3YcIWEMTamdTpDctoP6dOJz',\n",
              "  'name': 'multiply',\n",
              "  'type': 'function_call',\n",
              "  'id': 'fc_682501a2a7348198ab9d91d2d2cfb94c00c5f626929a51bd',\n",
              "  'status': 'completed'},\n",
              " {'arguments': '{\"x\":7.814,\"y\":1}',\n",
              "  'call_id': 'call_fsFqPlyKpDCc0uHpqxomzChn',\n",
              "  'name': 'multiply',\n",
              "  'type': 'function_call',\n",
              "  'id': 'fc_682501a2c41c8198be7ece857ca6d7ba00c5f626929a51bd',\n",
              "  'status': 'completed'},\n",
              " {'call_id': 'call_G3YcIWEMTamdTpDctoP6dOJz',\n",
              "  'output': '7.814',\n",
              "  'type': 'function_call_output'},\n",
              " {'call_id': 'call_fsFqPlyKpDCc0uHpqxomzChn',\n",
              "  'output': '7.814',\n",
              "  'type': 'function_call_output'},\n",
              " {'id': 'msg_682501a3ac20819884d6ffff4671c94b00c5f626929a51bd',\n",
              "  'content': [{'annotations': [],\n",
              "    'text': 'I have stored the number 7.814 for you.',\n",
              "    'type': 'output_text'}],\n",
              "  'role': 'assistant',\n",
              "  'status': 'completed',\n",
              "  'type': 'message'}]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We merge this with our next message:"
      ],
      "metadata": {
        "id": "Ia1n7UG_JkYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = await Runner.run(\n",
        "    starting_agent=agent,\n",
        "    input=result.to_input_list() + [\n",
        "        {\"role\": \"user\", \"content\": \"multiply the last number by 103.892\"}\n",
        "    ]\n",
        ")\n",
        "result.final_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "y4WKSTObJokR",
        "outputId": "006943cd-73f3-4e50-e522-546ee2b5de7c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The result of multiplying 7.814 by 103.892 is approximately 811.812088.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3kZHZne6WbkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like our agent can remember our previous interactions after all!"
      ],
      "metadata": {
        "id": "7bGdZMCLKDMK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "khGNoTd_KH4k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "That is our rapid-fire overview of OpenAI's new Agents SDK. We've covered most of the essentials here but there are many other features in the library, and many of the features we included here come with plenty of different ways to use. The SDK is already fairly substantial and certainly worth keeping an eye on."
      ],
      "metadata": {
        "id": "aFi__V9hKIKQ"
      }
    }
  ]
}